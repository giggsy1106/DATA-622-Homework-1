{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APgZ0oHv0h_2",
        "outputId": "3873ecbb-e1d5-47e6-dfd4-0dc63def51ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) HTTP status code for visitmaryland.org: 403\n",
            "\n",
            "2) First 1000 characters of visible text from visitmaryland.org:\n",
            "\n",
            "Enable JavaScript and cookies to continue\n",
            "Error fetching NLP Wikipedia page: 403 Client Error: Forbidden for url: https://en.wikipedia.org/wiki/Natural_language_processing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1883675230.py:23: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  texts = soup.find_all(text=True)\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4.element import Comment\n",
        "\n",
        "# 1. Retrieve the Maryland tourism web page and print HTTP status code\n",
        "url_md = \"https://www.visitmaryland.org/\"\n",
        "try:\n",
        "    resp_md = requests.get(url_md, timeout=20)\n",
        "    print(\"1) HTTP status code for visitmaryland.org:\", resp_md.status_code)\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(\"1) Error fetching visitmaryland.org:\", e)\n",
        "\n",
        "# 2. Extract visible text from the Maryland main page (ignore script/style)\n",
        "def tag_visible(element):\n",
        "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
        "        return False\n",
        "    if isinstance(element, Comment):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def visible_text_from_html(html):\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    texts = soup.find_all(text=True)\n",
        "    visible_texts = filter(tag_visible, texts)\n",
        "    # join and strip extra whitespace\n",
        "    return \" \".join(t.strip() for t in visible_texts if t.strip())\n",
        "\n",
        "if \"resp_md\" in locals() and isinstance(resp_md, requests.Response):\n",
        "    text_md = visible_text_from_html(resp_md.text)\n",
        "    print(\"\\n2) First 1000 characters of visible text from visitmaryland.org:\\n\")\n",
        "    print(text_md[:1000])  # just show a sample so itâ€™s not huge\n",
        "\n",
        "# 3. Fetch Wikipedia page for Natural Language Processing and extract headings\n",
        "url_nlp = \"https://en.wikipedia.org/wiki/Natural_language_processing\"\n",
        "try:\n",
        "    resp_nlp = requests.get(url_nlp, timeout=20)\n",
        "    resp_nlp.raise_for_status()\n",
        "    soup_nlp = BeautifulSoup(resp_nlp.text, \"html.parser\")\n",
        "\n",
        "    headings = []\n",
        "    for level in [\"h1\", \"h2\", \"h3\"]:\n",
        "        for h in soup_nlp.find_all(level):\n",
        "            headings.append((level, h.get_text(strip=True)))\n",
        "\n",
        "    print(\"\\n3) Headings (h1, h2, h3) from NLP Wikipedia page:\\n\")\n",
        "    for level, text in headings:\n",
        "        print(f\"{level}: {text}\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(\"Error fetching NLP Wikipedia page:\", e)\n",
        "\n",
        "# 4. Extract and print all URLs (href values) from the NLP Wikipedia page\n",
        "if \"soup_nlp\" in locals():\n",
        "    links = []\n",
        "    for a in soup_nlp.find_all(\"a\", href=True):\n",
        "        links.append(a[\"href\"])\n",
        "\n",
        "    print(\"\\n4) First 100 URLs from NLP Wikipedia page:\\n\")\n",
        "    for href in links[:100]:  # limit to first 100 so output is manageable\n",
        "        print(href)\n",
        "    print(f\"\\nTotal links found: {len(links)}\")\n",
        "\n",
        "# 5. Extract the first paragraph and save to nlp_intro.txt\n",
        "if \"soup_nlp\" in locals():\n",
        "    first_p = soup_nlp.find(\"p\")\n",
        "    if first_p:\n",
        "        first_p_text = first_p.get_text(strip=True)\n",
        "        with open(\"nlp_intro.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(first_p_text)\n",
        "        print(\"\\n5) Saved first paragraph to nlp_intro.txt\")\n",
        "        print(\"First paragraph preview:\\n\")\n",
        "        print(first_p_text)\n",
        "    else:\n",
        "        print(\"No <p> tag found on the NLP page.\")\n"
      ]
    }
  ]
}